#
# GPAW runtimes for the SCF-cycle (in seconds).
#

# Case 1: Carbon nanotube
#   A ground state calculation for a 6-6-10 carbon nanotube in vacuum
#   using serial LAPACK.
#
# Case 2: Copper filament
#   A ground state calculation for a copper filament (2x2x3 FCC lattice)
#   in vacuum using ScaLAPACK.
#

# Columns:
#   n   -- number of CPUs / GPUs / MICs
#   cpu -- runtime (in seconds) for CPUs
#          Intel Xeon Haswell (E5-2690v3)
#   knc -- runtime (in seconds) for KNCs
#          Intel Xeon Phi 7120P
#          (host) Intel Xeon Haswell (E5-2680v3)
#   knl -- runtime (in seconds) for KNLs
#          Intel Xeon Phi 7210
#   k40 -- runtime (in seconds) for K40 GPUs
#          NVIDIA Tesla K40
#          (host) Intel Xeon Ivy Bridge (E5-2620-v2)
#   k80 -- runtime (in seconds) for K40 GPUs
#          NVIDIA Tesla K80
#          (host) Intel Xeon Haswell (E5-2680v3)

#[ Carbon nanotube (6,6,10) ]
#: n cpu knc knl k40 k80
1 488.5 nan 319.9 237.6 249.4
2 242.2 281.2 206.6 nan nan
4 148.5 164.9 141.3 nan nan
8 81.1 102.9 101.3 nan nan
12 nan 87.0 nan nan nan
16 55.4 77.0 nan nan nan
24 nan 68.3 nan nan nan
32 32.7 63.8 nan nan nan
64 32.3 nan nan nan nan

#[ Copper filament (223,8) ]
# Note: the GPU times (k40, k80) are scaled up due to slightly different
#       run parameters (grid size 0.28 vs. the default 0.22)
#: n cpu knc knl k40 k80
1 748.9 nan 323.4 369.3 328.9
2 405.0 nan 172.3 nan nan
4 191.5 nan 127.0 nan nan
8 86.9 102.6 80.0 nan nan
12 nan nan nan nan nan
16 60.5 69.4 nan nan nan
24 nan 53.8 nan nan nan
32 32.3 42.8 nan nan nan
64 19.4 31.7 nan nan nan


#
# Case 2 (copper filament) didn't have enough memory on GPUs using default
# parameters. Grid size was increased from 0.22 to 0.28 to alleviate this.
# For the comparison, the results need to be scaled up to account for this
# using the CPU result as a yardstick (scaling factor q=2.018). Below are
# the raw numbers for both CPUs and GPUs.
#
#[ Copper filament (223,8 / grid 0.28) ]
#: n cpu knc knl k40 k80
1 371.2 nan nan 183.0 163.0
